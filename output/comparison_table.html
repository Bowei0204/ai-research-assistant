<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>æ¨™é¡Œ</th>
      <th>ä½œè€…</th>
      <th>æ–¹æ³•æ‘˜è¦</th>
      <th>å„ªé»èˆ‡é™åˆ¶</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>A Principled Bayesian Framework for Training Binary and Spiking Neural\n  Networks\n\n- **ä½œè€…**ï¼šJames A. Walker, Moein Khajehnejad, Adeel Razi\n- **ç™¼è¡¨æ—¥æœŸ**ï¼š2025-05-23\n- **arXiv ID**ï¼š2505.17962\n\n---\n\n## ğŸ§  Abstractï¼ˆè‹±æ–‡ï¼‰\n\nWe propose a Bayesian framework for training binary and spiking neural networks that achieves state-of-the-art performance without normalisation layers. Unlike commonly used surrogate gradient methods -- often heuristic and sensitive to hyperparameter choices -- our approach is grounded in a probabilistic model of noisy binary networks, enabling fully end-to-end gradient-based optimisation. We introduce importance-weighted straight-through (IW-ST) estimators, a unified class generalising straight-through and relaxation-based estimators. We characterise the bias-variance trade-off in this family and derive a bias-minimising objective implemented via an auxiliary loss. Building on this, we introduce Spiking Bayesian Neural Networks (SBNNs), a variational inference framework that uses posterior noise to train Binary and Spiking Neural Networks with IW-ST. This Bayesian approach minimises gradient bias, regularises parameters, and introduces dropout-like noise. By linking low-bias conditions, vanishing gradients, and the KL term, we enable training of deep residual networks without normalisation. Experiments on CIFAR-10, DVS Gesture, and SHD show our method matches or exceeds existing approaches without normalisation or hand-tuned gradients.\n\n## ğŸŒ Abstractï¼ˆç¹é«”ç¿»è­¯ï¼‰\n\næˆ‘å€‘æå‡ºäº†ä¸€å€‹è²è‘‰æ–¯æ¡†æ¶ï¼Œç”¨æ–¼è¨“ç·´äºŒé€²åˆ¶å’Œå°–å³°ç¥ç¶“ç¶²çµ¡ï¼Œè©²æ¡†æ¶åœ¨æ²’æœ‰æ¨™æº–åŒ–å±¤çš„æƒ…æ³ä¸‹å¯¦ç¾æœ€å…ˆé€²çš„æ€§èƒ½ã€‚èˆ‡å¸¸ç”¨çš„æ›¿ä»£æ¢¯åº¦æ–¹æ³•ï¼ˆé€šå¸¸æ˜¯å•Ÿç™¼å¼ä¸”å°è¶…åƒæ•¸é¸æ“‡æ•æ„Ÿï¼‰ä¸åŒï¼Œæˆ‘å€‘çš„æ–¹æ³•åŸºæ–¼å˜ˆé›œçš„äºŒé€²è£½ç¶²çµ¡çš„æ¦‚ç‡æ¨¡å‹ï¼Œå¾è€Œå¯¦ç¾äº†å®Œå…¨åŸºæ–¼ç«¯åˆ°ç«¯æ¢¯åº¦çš„å„ªåŒ–ã€‚æˆ‘å€‘ä»‹ç´¹äº†é‡è¦æ€§åŠ æ¬Šçš„ç›´ç™¼ï¼ˆIW-STï¼‰ä¼°è¨ˆå™¨ï¼Œé€™æ˜¯ä¸€ç¨®çµ±ä¸€çš„é¡åˆ¥çš„æ¦‚æ‹¬æ€§ç›´é€šå’ŒåŸºæ–¼æ”¾é¬†çš„ä¼°è¨ˆå™¨ã€‚æˆ‘å€‘è¡¨å¾µäº†é€™å€‹å®¶åº­ä¸­çš„åè¦‹è®ŠåŒ–æ¬Šè¡¡æ¬Šè¡¡ï¼Œä¸¦é€šéè¼”åŠ©æå¤±å¾—å‡ºäº†å¯¦æ–½çš„åè¦‹æœ€å°ç›®æ¨™ã€‚åœ¨æ­¤åŸºç¤ä¸Šï¼Œæˆ‘å€‘å¼•å…¥äº†å°–å³°è²è‘‰æ–¯ç¥ç¶“ç¶²çµ¡ï¼ˆSBNNSï¼‰ï¼Œé€™æ˜¯ä¸€å€‹è®Šç•°æ¨ç†æ¡†æ¶ï¼Œä½¿ç”¨å¾Œå™ªè²è¨“ç·´èˆ‡IW-STè¨“ç·´äºŒé€²åˆ¶å’Œå°–å³°ç¥ç¶“ç¶²çµ¡ã€‚é€™ç¨®è²è‘‰æ–¯æ–¹æ³•å¯ä»¥æœ€å¤§ç¨‹åº¦åœ°æ¸›å°‘æ¢¯åº¦åå·®ï¼Œè¦ç¯„åƒæ•¸ä¸¦å¼•å…¥é¡ä¼¼è¼Ÿå­¸çš„å™ªè²ã€‚é€šéé€£æ¥ä½åç½®æ¢ä»¶ï¼Œæ¶ˆå¤±çš„æ¢¯åº¦å’ŒKLæœŸé™ï¼Œæˆ‘å€‘å¯ä»¥åŸ¹è¨“æ·±å±¤æ®˜ç•™ç¶²çµ¡è€Œç„¡éœ€æ­¸ä¸€åŒ–ã€‚åœ¨CIFAR-10ï¼ŒDVSæ‰‹å‹¢å’ŒSHDä¸Šé€²è¡Œçš„å¯¦é©—é¡¯ç¤ºäº†æˆ‘å€‘çš„æ–¹æ³•åŒ¹é…æˆ–è¶…éäº†æ²’æœ‰æ¨™æº–åŒ–æˆ–æ‰‹å‹•æ¢¯åº¦çš„ç¾æœ‰æ–¹æ³•ã€‚\n\n---\n\n## âœ… å„ªé»\n- **å¯¦ç¾æœ€å…ˆé€²çš„æ€§èƒ½ï¼Œç„¡éœ€æ­¸ä¸€åŒ–å±¤:**  é€™æ˜¯è©²æ–¹æ³•çš„ä¸»è¦å„ªå‹¢ï¼Œå…‹æœäº†è¨±å¤šæ·±åº¦å­¸ç¿’æ¨¡å‹å°æ­¸ä¸€åŒ–å±¤çš„ä¾è³´ã€‚\n- **åŸºæ–¼æ¦‚ç‡æ¨¡å‹ï¼Œè€Œéå•Ÿç™¼å¼æ–¹æ³•:** èˆ‡å¸¸ç”¨çš„æ›¿ä»£æ¢¯åº¦æ–¹æ³•ä¸åŒï¼Œè©²æ–¹æ³•åŸºæ–¼å°å¸¶å™ªè²äºŒå…ƒç¶²çµ¡çš„æ¦‚ç‡æ¨¡å‹ï¼Œä½¿å¾—ç«¯åˆ°ç«¯æ¢¯åº¦å„ªåŒ–æˆç‚ºå¯èƒ½ï¼Œæ›´å…·ç†è«–åŸºç¤ã€‚\n- **å¼•å…¥çµ±ä¸€çš„IW-STä¼°è¨ˆå™¨:**  æ­¤ä¼°è¨ˆå™¨æ³›åŒ–äº†ç›´é€šä¼°è¨ˆå™¨å’ŒåŸºæ–¼é¬†å¼›çš„ä¼°è¨ˆå™¨ï¼Œæä¾›äº†ä¸€å€‹æ›´é€šç”¨çš„æ¡†æ¶ã€‚\n- **ç‰¹å¾µåŒ–åå·®-æ–¹å·®æ¬Šè¡¡ä¸¦æœ€å°åŒ–åå·®:**  é€šéåˆ†æåå·®-æ–¹å·®æ¬Šè¡¡ä¸¦å¼•å…¥è¼”åŠ©æå¤±å‡½æ•¸ï¼Œæ¸›å°‘äº†æ¢¯åº¦ä¼°è¨ˆçš„åå·®ã€‚\n- **æˆåŠŸæ‡‰ç”¨æ–¼äºŒå…ƒå’Œè„ˆè¡ç¥ç¶“ç¶²çµ¡ (SBNNs):**  è©²æ–¹æ³•ä¸å±€é™æ–¼äºŒå…ƒç¶²çµ¡ï¼Œä¹ŸæˆåŠŸæ‡‰ç”¨æ–¼æ›´è¤‡é›œçš„è„ˆè¡ç¥ç¶“ç¶²çµ¡ã€‚\n- **è²è‘‰æ–¯æ–¹æ³•å¸¶ä¾†æ­£å‰‡åŒ–å’Œdropout-like noise:**  è²è‘‰æ–¯æ–¹æ³•å…§åœ¨çš„æ­£å‰‡åŒ–ç‰¹æ€§æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸¦ä¸”å¼•å…¥äº†é¡ä¼¼dropoutçš„å™ªè²ï¼Œæœ‰åŠ©æ–¼é˜²æ­¢éæ“¬åˆã€‚\n- **åœ¨å¤šå€‹æ•¸æ“šé›†ä¸Šå–å¾—äº†å„ªç§€çš„çµæœ:** åœ¨CIFAR-10, DVS Gestureå’ŒSHDæ•¸æ“šé›†ä¸Šéƒ½å–å¾—äº†èˆ‡ç¾æœ‰æ–¹æ³•ç›¸ç•¶æˆ–æ›´å¥½çš„çµæœã€‚\n\n\n## âš ï¸ ç¼ºé» / é™åˆ¶\n- **è¨ˆç®—è¤‡é›œåº¦:**  è²è‘‰æ–¯æ–¹æ³•é€šå¸¸æ¯”å‚³çµ±æ–¹æ³•è¨ˆç®—è¤‡é›œåº¦æ›´é«˜ï¼Œé€™å¯èƒ½æœƒé™åˆ¶å…¶æ‡‰ç”¨æ–¼å¤§å‹æ•¸æ“šé›†æˆ–æ›´è¤‡é›œçš„ç¶²çµ¡çµæ§‹ã€‚\n- **è¶…åƒæ•¸é¸æ“‡:**  é›–ç„¶æ–‡ä¸­æåˆ°é¿å…äº†æ›¿ä»£æ¢¯åº¦æ–¹æ³•ä¸­å°è¶…åƒæ•¸çš„æ•æ„Ÿæ€§ï¼Œä½†è©²æ–¹æ³•æœ¬èº«å¯èƒ½ä»ç„¶å­˜åœ¨éœ€è¦èª¿æ•´çš„è¶…åƒæ•¸ï¼Œåªæ˜¯æ•¸é‡å¯èƒ½è¼ƒå°‘ã€‚\n- **å¯æ“´å±•æ€§:**  é›–ç„¶åœ¨æ–‡ä¸­æåˆ°çš„æ•¸æ“šé›†ä¸Šå–å¾—äº†æˆåŠŸï¼Œä½†è©²æ–¹æ³•çš„å¯æ“´å±•æ€§åœ¨æ›´å¤§è¦æ¨¡çš„æ•¸æ“šé›†å’Œæ›´è¤‡é›œçš„ä»»å‹™ä¸Šä»éœ€é©—è­‰ã€‚\n- **å°å…ˆé©—åˆ†ä½ˆçš„é¸æ“‡:** è²è‘‰æ–¯æ–¹æ³•çš„æ€§èƒ½é«˜åº¦ä¾è³´æ–¼å…ˆé©—åˆ†ä½ˆçš„é¸æ“‡ï¼Œä¸æ°ç•¶çš„å…ˆé©—åˆ†ä½ˆå¯èƒ½æœƒå½±éŸ¿æ¨¡å‹çš„æ€§èƒ½ã€‚\n- **ç¼ºä¹å°IW-STä¼°è¨ˆå™¨åå·®æœ€å°åŒ–æ–¹æ³•çš„æ·±å…¥è§£é‡‹:** æ‘˜è¦ä¸­åªç°¡è¦æåˆ°äº†åå·®æœ€å°åŒ–ï¼Œç¼ºä¹æ›´è©³ç´°çš„ç†è«–åˆ†æå’Œè§£é‡‹ã€‚\n\n---\nï¼ˆç”± AI æŠ€è¡“ç ”ç©¶åŠ©æ‰‹è‡ªå‹•ç”¢ç”Ÿï¼‰</td>
      <td>James A. Walker, Moein Khajehnejad, Adeel Razi\n- **ç™¼è¡¨æ—¥æœŸ**ï¼š2025-05-23\n- **arXiv ID**ï¼š2505.17962\n\n---\n\n## ğŸ§  Abstractï¼ˆè‹±æ–‡ï¼‰\n\nWe propose a Bayesian framework for training binary and spiking neural networks that achieves state-of-the-art performance without normalisation layers. Unlike commonly used surrogate gradient methods -- often heuristic and sensitive to hyperparameter choices -- our approach is grounded in a probabilistic model of noisy binary networks, enabling fully end-to-end gradient-based optimisation. We introduce importance-weighted straight-through (IW-ST) estimators, a unified class generalising straight-through and relaxation-based estimators. We characterise the bias-variance trade-off in this family and derive a bias-minimising objective implemented via an auxiliary loss. Building on this, we introduce Spiking Bayesian Neural Networks (SBNNs), a variational inference framework that uses posterior noise to train Binary and Spiking Neural Networks with IW-ST. This Bayesian approach minimises gradient bias, regularises parameters, and introduces dropout-like noise. By linking low-bias conditions, vanishing gradients, and the KL term, we enable training of deep residual networks without normalisation. Experiments on CIFAR-10, DVS Gesture, and SHD show our method matches or exceeds existing approaches without normalisation or hand-tuned gradients.\n\n## ğŸŒ Abstractï¼ˆç¹é«”ç¿»è­¯ï¼‰\n\næˆ‘å€‘æå‡ºäº†ä¸€å€‹è²è‘‰æ–¯æ¡†æ¶ï¼Œç”¨æ–¼è¨“ç·´äºŒé€²åˆ¶å’Œå°–å³°ç¥ç¶“ç¶²çµ¡ï¼Œè©²æ¡†æ¶åœ¨æ²’æœ‰æ¨™æº–åŒ–å±¤çš„æƒ…æ³ä¸‹å¯¦ç¾æœ€å…ˆé€²çš„æ€§èƒ½ã€‚èˆ‡å¸¸ç”¨çš„æ›¿ä»£æ¢¯åº¦æ–¹æ³•ï¼ˆé€šå¸¸æ˜¯å•Ÿç™¼å¼ä¸”å°è¶…åƒæ•¸é¸æ“‡æ•æ„Ÿï¼‰ä¸åŒï¼Œæˆ‘å€‘çš„æ–¹æ³•åŸºæ–¼å˜ˆé›œçš„äºŒé€²è£½ç¶²çµ¡çš„æ¦‚ç‡æ¨¡å‹ï¼Œå¾è€Œå¯¦ç¾äº†å®Œå…¨åŸºæ–¼ç«¯åˆ°ç«¯æ¢¯åº¦çš„å„ªåŒ–ã€‚æˆ‘å€‘ä»‹ç´¹äº†é‡è¦æ€§åŠ æ¬Šçš„ç›´ç™¼ï¼ˆIW-STï¼‰ä¼°è¨ˆå™¨ï¼Œé€™æ˜¯ä¸€ç¨®çµ±ä¸€çš„é¡åˆ¥çš„æ¦‚æ‹¬æ€§ç›´é€šå’ŒåŸºæ–¼æ”¾é¬†çš„ä¼°è¨ˆå™¨ã€‚æˆ‘å€‘è¡¨å¾µäº†é€™å€‹å®¶åº­ä¸­çš„åè¦‹è®ŠåŒ–æ¬Šè¡¡æ¬Šè¡¡ï¼Œä¸¦é€šéè¼”åŠ©æå¤±å¾—å‡ºäº†å¯¦æ–½çš„åè¦‹æœ€å°ç›®æ¨™ã€‚åœ¨æ­¤åŸºç¤ä¸Šï¼Œæˆ‘å€‘å¼•å…¥äº†å°–å³°è²è‘‰æ–¯ç¥ç¶“ç¶²çµ¡ï¼ˆSBNNSï¼‰ï¼Œé€™æ˜¯ä¸€å€‹è®Šç•°æ¨ç†æ¡†æ¶ï¼Œä½¿ç”¨å¾Œå™ªè²è¨“ç·´èˆ‡IW-STè¨“ç·´äºŒé€²åˆ¶å’Œå°–å³°ç¥ç¶“ç¶²çµ¡ã€‚é€™ç¨®è²è‘‰æ–¯æ–¹æ³•å¯ä»¥æœ€å¤§ç¨‹åº¦åœ°æ¸›å°‘æ¢¯åº¦åå·®ï¼Œè¦ç¯„åƒæ•¸ä¸¦å¼•å…¥é¡ä¼¼è¼Ÿå­¸çš„å™ªè²ã€‚é€šéé€£æ¥ä½åç½®æ¢ä»¶ï¼Œæ¶ˆå¤±çš„æ¢¯åº¦å’ŒKLæœŸé™ï¼Œæˆ‘å€‘å¯ä»¥åŸ¹è¨“æ·±å±¤æ®˜ç•™ç¶²çµ¡è€Œç„¡éœ€æ­¸ä¸€åŒ–ã€‚åœ¨CIFAR-10ï¼ŒDVSæ‰‹å‹¢å’ŒSHDä¸Šé€²è¡Œçš„å¯¦é©—é¡¯ç¤ºäº†æˆ‘å€‘çš„æ–¹æ³•åŒ¹é…æˆ–è¶…éäº†æ²’æœ‰æ¨™æº–åŒ–æˆ–æ‰‹å‹•æ¢¯åº¦çš„ç¾æœ‰æ–¹æ³•ã€‚\n\n---\n\n## âœ… å„ªé»\n- **å¯¦ç¾æœ€å…ˆé€²çš„æ€§èƒ½ï¼Œç„¡éœ€æ­¸ä¸€åŒ–å±¤:**  é€™æ˜¯è©²æ–¹æ³•çš„ä¸»è¦å„ªå‹¢ï¼Œå…‹æœäº†è¨±å¤šæ·±åº¦å­¸ç¿’æ¨¡å‹å°æ­¸ä¸€åŒ–å±¤çš„ä¾è³´ã€‚\n- **åŸºæ–¼æ¦‚ç‡æ¨¡å‹ï¼Œè€Œéå•Ÿç™¼å¼æ–¹æ³•:** èˆ‡å¸¸ç”¨çš„æ›¿ä»£æ¢¯åº¦æ–¹æ³•ä¸åŒï¼Œè©²æ–¹æ³•åŸºæ–¼å°å¸¶å™ªè²äºŒå…ƒç¶²çµ¡çš„æ¦‚ç‡æ¨¡å‹ï¼Œä½¿å¾—ç«¯åˆ°ç«¯æ¢¯åº¦å„ªåŒ–æˆç‚ºå¯èƒ½ï¼Œæ›´å…·ç†è«–åŸºç¤ã€‚\n- **å¼•å…¥çµ±ä¸€çš„IW-STä¼°è¨ˆå™¨:**  æ­¤ä¼°è¨ˆå™¨æ³›åŒ–äº†ç›´é€šä¼°è¨ˆå™¨å’ŒåŸºæ–¼é¬†å¼›çš„ä¼°è¨ˆå™¨ï¼Œæä¾›äº†ä¸€å€‹æ›´é€šç”¨çš„æ¡†æ¶ã€‚\n- **ç‰¹å¾µåŒ–åå·®-æ–¹å·®æ¬Šè¡¡ä¸¦æœ€å°åŒ–åå·®:**  é€šéåˆ†æåå·®-æ–¹å·®æ¬Šè¡¡ä¸¦å¼•å…¥è¼”åŠ©æå¤±å‡½æ•¸ï¼Œæ¸›å°‘äº†æ¢¯åº¦ä¼°è¨ˆçš„åå·®ã€‚\n- **æˆåŠŸæ‡‰ç”¨æ–¼äºŒå…ƒå’Œè„ˆè¡ç¥ç¶“ç¶²çµ¡ (SBNNs):**  è©²æ–¹æ³•ä¸å±€é™æ–¼äºŒå…ƒç¶²çµ¡ï¼Œä¹ŸæˆåŠŸæ‡‰ç”¨æ–¼æ›´è¤‡é›œçš„è„ˆè¡ç¥ç¶“ç¶²çµ¡ã€‚\n- **è²è‘‰æ–¯æ–¹æ³•å¸¶ä¾†æ­£å‰‡åŒ–å’Œdropout-like noise:**  è²è‘‰æ–¯æ–¹æ³•å…§åœ¨çš„æ­£å‰‡åŒ–ç‰¹æ€§æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸¦ä¸”å¼•å…¥äº†é¡ä¼¼dropoutçš„å™ªè²ï¼Œæœ‰åŠ©æ–¼é˜²æ­¢éæ“¬åˆã€‚\n- **åœ¨å¤šå€‹æ•¸æ“šé›†ä¸Šå–å¾—äº†å„ªç§€çš„çµæœ:** åœ¨CIFAR-10, DVS Gestureå’ŒSHDæ•¸æ“šé›†ä¸Šéƒ½å–å¾—äº†èˆ‡ç¾æœ‰æ–¹æ³•ç›¸ç•¶æˆ–æ›´å¥½çš„çµæœã€‚\n\n\n## âš ï¸ ç¼ºé» / é™åˆ¶\n- **è¨ˆç®—è¤‡é›œåº¦:**  è²è‘‰æ–¯æ–¹æ³•é€šå¸¸æ¯”å‚³çµ±æ–¹æ³•è¨ˆç®—è¤‡é›œåº¦æ›´é«˜ï¼Œé€™å¯èƒ½æœƒé™åˆ¶å…¶æ‡‰ç”¨æ–¼å¤§å‹æ•¸æ“šé›†æˆ–æ›´è¤‡é›œçš„ç¶²çµ¡çµæ§‹ã€‚\n- **è¶…åƒæ•¸é¸æ“‡:**  é›–ç„¶æ–‡ä¸­æåˆ°é¿å…äº†æ›¿ä»£æ¢¯åº¦æ–¹æ³•ä¸­å°è¶…åƒæ•¸çš„æ•æ„Ÿæ€§ï¼Œä½†è©²æ–¹æ³•æœ¬èº«å¯èƒ½ä»ç„¶å­˜åœ¨éœ€è¦èª¿æ•´çš„è¶…åƒæ•¸ï¼Œåªæ˜¯æ•¸é‡å¯èƒ½è¼ƒå°‘ã€‚\n- **å¯æ“´å±•æ€§:**  é›–ç„¶åœ¨æ–‡ä¸­æåˆ°çš„æ•¸æ“šé›†ä¸Šå–å¾—äº†æˆåŠŸï¼Œä½†è©²æ–¹æ³•çš„å¯æ“´å±•æ€§åœ¨æ›´å¤§è¦æ¨¡çš„æ•¸æ“šé›†å’Œæ›´è¤‡é›œçš„ä»»å‹™ä¸Šä»éœ€é©—è­‰ã€‚\n- **å°å…ˆé©—åˆ†ä½ˆçš„é¸æ“‡:** è²è‘‰æ–¯æ–¹æ³•çš„æ€§èƒ½é«˜åº¦ä¾è³´æ–¼å…ˆé©—åˆ†ä½ˆçš„é¸æ“‡ï¼Œä¸æ°ç•¶çš„å…ˆé©—åˆ†ä½ˆå¯èƒ½æœƒå½±éŸ¿æ¨¡å‹çš„æ€§èƒ½ã€‚\n- **ç¼ºä¹å°IW-STä¼°è¨ˆå™¨åå·®æœ€å°åŒ–æ–¹æ³•çš„æ·±å…¥è§£é‡‹:** æ‘˜è¦ä¸­åªç°¡è¦æåˆ°äº†åå·®æœ€å°åŒ–ï¼Œç¼ºä¹æ›´è©³ç´°çš„ç†è«–åˆ†æå’Œè§£é‡‹ã€‚\n\n---\nï¼ˆç”± AI æŠ€è¡“ç ”ç©¶åŠ©æ‰‹è‡ªå‹•ç”¢ç”Ÿï¼‰</td>
      <td>We propose a Bayesian framework for training binary and spiking neural networks that achieves state-of-the-art performance without normalisation layers. Unlike commonly used surrogate gradient methods -- often heuristic and sensitive to hyperparameter choices -- our approach is grounded in a probabilistic model of noisy binary networks, enabling fully end-to-end gradient-based optimisation. We introduce importance-weighted straight-through (IW-ST) estimators, a unified class generalising straight-through and relaxation-based estimators. We characterise the bias-variance trade-off in this family and derive a bias-minimising objective implemented via an auxiliary loss. Building on this, we introduce Spiking Bayesian Neural Networks (SBNNs), a variational inference framework that uses posterior noise to train Binary and Spiking Neural Networks with IW-ST. This Bayesian approach minimises gradient bias, regularises parameters, and introduces dropout-like noise. By linking low-bias conditions, vanishing gradients, and the KL term, we enable training of deep residual networks without normalisation. Experiments on CIFAR-10, DVS Gesture, and SHD show our method matches or exceeds existing approaches without normalisation or hand-tuned gradients.</td>
      <td>## âœ… å„ªé»\n- **å¯¦ç¾æœ€å…ˆé€²çš„æ€§èƒ½ï¼Œç„¡éœ€æ­¸ä¸€åŒ–å±¤:**  é€™æ˜¯è©²æ–¹æ³•çš„ä¸»è¦å„ªå‹¢ï¼Œå…‹æœäº†è¨±å¤šæ·±åº¦å­¸ç¿’æ¨¡å‹å°æ­¸ä¸€åŒ–å±¤çš„ä¾è³´ã€‚\n- **åŸºæ–¼æ¦‚ç‡æ¨¡å‹ï¼Œè€Œéå•Ÿç™¼å¼æ–¹æ³•:** èˆ‡å¸¸ç”¨çš„æ›¿ä»£æ¢¯åº¦æ–¹æ³•ä¸åŒï¼Œè©²æ–¹æ³•åŸºæ–¼å°å¸¶å™ªè²äºŒå…ƒç¶²çµ¡çš„æ¦‚ç‡æ¨¡å‹ï¼Œä½¿å¾—ç«¯åˆ°ç«¯æ¢¯åº¦å„ªåŒ–æˆç‚ºå¯èƒ½ï¼Œæ›´å…·ç†è«–åŸºç¤ã€‚\n- **å¼•å…¥çµ±ä¸€çš„IW-STä¼°è¨ˆå™¨:**  æ­¤ä¼°è¨ˆå™¨æ³›åŒ–äº†ç›´é€šä¼°è¨ˆå™¨å’ŒåŸºæ–¼é¬†å¼›çš„ä¼°è¨ˆå™¨ï¼Œæä¾›äº†ä¸€å€‹æ›´é€šç”¨çš„æ¡†æ¶ã€‚\n- **ç‰¹å¾µåŒ–åå·®-æ–¹å·®æ¬Šè¡¡ä¸¦æœ€å°åŒ–åå·®:**  é€šéåˆ†æåå·®-æ–¹å·®æ¬Šè¡¡ä¸¦å¼•å…¥è¼”åŠ©æå¤±å‡½æ•¸ï¼Œæ¸›å°‘äº†æ¢¯åº¦ä¼°è¨ˆçš„åå·®ã€‚\n- **æˆåŠŸæ‡‰ç”¨æ–¼äºŒå…ƒå’Œè„ˆè¡ç¥ç¶“ç¶²çµ¡ (SBNNs):**  è©²æ–¹æ³•ä¸å±€é™æ–¼äºŒå…ƒç¶²çµ¡ï¼Œä¹ŸæˆåŠŸæ‡‰ç”¨æ–¼æ›´è¤‡é›œçš„è„ˆè¡ç¥ç¶“ç¶²çµ¡ã€‚\n- **è²è‘‰æ–¯æ–¹æ³•å¸¶ä¾†æ­£å‰‡åŒ–å’Œdropout-like noise:**  è²è‘‰æ–¯æ–¹æ³•å…§åœ¨çš„æ­£å‰‡åŒ–ç‰¹æ€§æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸¦ä¸”å¼•å…¥äº†é¡ä¼¼dropoutçš„å™ªè²ï¼Œæœ‰åŠ©æ–¼é˜²æ­¢éæ“¬åˆã€‚\n- **åœ¨å¤šå€‹æ•¸æ“šé›†ä¸Šå–å¾—äº†å„ªç§€çš„çµæœ:** åœ¨CIFAR-10, DVS Gestureå’ŒSHDæ•¸æ“šé›†ä¸Šéƒ½å–å¾—äº†èˆ‡ç¾æœ‰æ–¹æ³•ç›¸ç•¶æˆ–æ›´å¥½çš„çµæœã€‚\n\n\n## âš ï¸ ç¼ºé» / é™åˆ¶\n- **è¨ˆç®—è¤‡é›œåº¦:**  è²è‘‰æ–¯æ–¹æ³•é€šå¸¸æ¯”å‚³çµ±æ–¹æ³•è¨ˆç®—è¤‡é›œåº¦æ›´é«˜ï¼Œé€™å¯èƒ½æœƒé™åˆ¶å…¶æ‡‰ç”¨æ–¼å¤§å‹æ•¸æ“šé›†æˆ–æ›´è¤‡é›œçš„ç¶²çµ¡çµæ§‹ã€‚\n- **è¶…åƒæ•¸é¸æ“‡:**  é›–ç„¶æ–‡ä¸­æåˆ°é¿å…äº†æ›¿ä»£æ¢¯åº¦æ–¹æ³•ä¸­å°è¶…åƒæ•¸çš„æ•æ„Ÿæ€§ï¼Œä½†è©²æ–¹æ³•æœ¬èº«å¯èƒ½ä»ç„¶å­˜åœ¨éœ€è¦èª¿æ•´çš„è¶…åƒæ•¸ï¼Œåªæ˜¯æ•¸é‡å¯èƒ½è¼ƒå°‘ã€‚\n- **å¯æ“´å±•æ€§:**  é›–ç„¶åœ¨æ–‡ä¸­æåˆ°çš„æ•¸æ“šé›†ä¸Šå–å¾—äº†æˆåŠŸï¼Œä½†è©²æ–¹æ³•çš„å¯æ“´å±•æ€§åœ¨æ›´å¤§è¦æ¨¡çš„æ•¸æ“šé›†å’Œæ›´è¤‡é›œçš„ä»»å‹™ä¸Šä»éœ€é©—è­‰ã€‚\n- **å°å…ˆé©—åˆ†ä½ˆçš„é¸æ“‡:** è²è‘‰æ–¯æ–¹æ³•çš„æ€§èƒ½é«˜åº¦ä¾è³´æ–¼å…ˆé©—åˆ†ä½ˆçš„é¸æ“‡ï¼Œä¸æ°ç•¶çš„å…ˆé©—åˆ†ä½ˆå¯èƒ½æœƒå½±éŸ¿æ¨¡å‹çš„æ€§èƒ½ã€‚\n- **ç¼ºä¹å°IW-STä¼°è¨ˆå™¨åå·®æœ€å°åŒ–æ–¹æ³•çš„æ·±å…¥è§£é‡‹:** æ‘˜è¦ä¸­åªç°¡è¦æåˆ°äº†åå·®æœ€å°åŒ–ï¼Œç¼ºä¹æ›´è©³ç´°çš„ç†è«–åˆ†æå’Œè§£é‡‹ã€‚</td>
    </tr>
  </tbody>
</table>