# æŠ€è¡“æ¯”è¼ƒè¡¨

## A Principled Bayesian Framework for Training Binary and Spiking Neural
  Networks

- **ä½œè€…**ï¼šJames A. Walker, Moein Khajehnejad, Adeel Razi
- **ç™¼è¡¨æ—¥æœŸ**ï¼š2025-05-23
- **arXiv ID**ï¼š2505.17962

---

## ğŸ§  Abstractï¼ˆè‹±æ–‡ï¼‰

We propose a Bayesian framework for training binary and spiking neural networks that achieves state-of-the-art performance without normalisation layers. Unlike commonly used surrogate gradient methods -- often heuristic and sensitive to hyperparameter choices -- our approach is grounded in a probabilistic model of noisy binary networks, enabling fully end-to-end gradient-based optimisation. We introduce importance-weighted straight-through (IW-ST) estimators, a unified class generalising straight-through and relaxation-based estimators. We characterise the bias-variance trade-off in this family and derive a bias-minimising objective implemented via an auxiliary loss. Building on this, we introduce Spiking Bayesian Neural Networks (SBNNs), a variational inference framework that uses posterior noise to train Binary and Spiking Neural Networks with IW-ST. This Bayesian approach minimises gradient bias, regularises parameters, and introduces dropout-like noise. By linking low-bias conditions, vanishing gradients, and the KL term, we enable training of deep residual networks without normalisation. Experiments on CIFAR-10, DVS Gesture, and SHD show our method matches or exceeds existing approaches without normalisation or hand-tuned gradients.

## ğŸŒ Abstractï¼ˆç¹é«”ç¿»è­¯ï¼‰

æˆ‘å€‘æå‡ºäº†ä¸€å€‹è²è‘‰æ–¯æ¡†æ¶ï¼Œç”¨æ–¼è¨“ç·´äºŒé€²åˆ¶å’Œå°–å³°ç¥ç¶“ç¶²çµ¡ï¼Œè©²æ¡†æ¶åœ¨æ²’æœ‰æ¨™æº–åŒ–å±¤çš„æƒ…æ³ä¸‹å¯¦ç¾æœ€å…ˆé€²çš„æ€§èƒ½ã€‚èˆ‡å¸¸ç”¨çš„æ›¿ä»£æ¢¯åº¦æ–¹æ³•ï¼ˆé€šå¸¸æ˜¯å•Ÿç™¼å¼ä¸”å°è¶…åƒæ•¸é¸æ“‡æ•æ„Ÿï¼‰ä¸åŒï¼Œæˆ‘å€‘çš„æ–¹æ³•åŸºæ–¼å˜ˆé›œçš„äºŒé€²è£½ç¶²çµ¡çš„æ¦‚ç‡æ¨¡å‹ï¼Œå¾è€Œå¯¦ç¾äº†å®Œå…¨åŸºæ–¼ç«¯åˆ°ç«¯æ¢¯åº¦çš„å„ªåŒ–ã€‚æˆ‘å€‘ä»‹ç´¹äº†é‡è¦æ€§åŠ æ¬Šçš„ç›´ç™¼ï¼ˆIW-STï¼‰ä¼°è¨ˆå™¨ï¼Œé€™æ˜¯ä¸€ç¨®çµ±ä¸€çš„é¡åˆ¥çš„æ¦‚æ‹¬æ€§ç›´é€šå’ŒåŸºæ–¼æ”¾é¬†çš„ä¼°è¨ˆå™¨ã€‚æˆ‘å€‘è¡¨å¾µäº†é€™å€‹å®¶åº­ä¸­çš„åè¦‹è®ŠåŒ–æ¬Šè¡¡æ¬Šè¡¡ï¼Œä¸¦é€šéè¼”åŠ©æå¤±å¾—å‡ºäº†å¯¦æ–½çš„åè¦‹æœ€å°ç›®æ¨™ã€‚åœ¨æ­¤åŸºç¤ä¸Šï¼Œæˆ‘å€‘å¼•å…¥äº†å°–å³°è²è‘‰æ–¯ç¥ç¶“ç¶²çµ¡ï¼ˆSBNNSï¼‰ï¼Œé€™æ˜¯ä¸€å€‹è®Šç•°æ¨ç†æ¡†æ¶ï¼Œä½¿ç”¨å¾Œå™ªè²è¨“ç·´èˆ‡IW-STè¨“ç·´äºŒé€²åˆ¶å’Œå°–å³°ç¥ç¶“ç¶²çµ¡ã€‚é€™ç¨®è²è‘‰æ–¯æ–¹æ³•å¯ä»¥æœ€å¤§ç¨‹åº¦åœ°æ¸›å°‘æ¢¯åº¦åå·®ï¼Œè¦ç¯„åƒæ•¸ä¸¦å¼•å…¥é¡ä¼¼è¼Ÿå­¸çš„å™ªè²ã€‚é€šéé€£æ¥ä½åç½®æ¢ä»¶ï¼Œæ¶ˆå¤±çš„æ¢¯åº¦å’ŒKLæœŸé™ï¼Œæˆ‘å€‘å¯ä»¥åŸ¹è¨“æ·±å±¤æ®˜ç•™ç¶²çµ¡è€Œç„¡éœ€æ­¸ä¸€åŒ–ã€‚åœ¨CIFAR-10ï¼ŒDVSæ‰‹å‹¢å’ŒSHDä¸Šé€²è¡Œçš„å¯¦é©—é¡¯ç¤ºäº†æˆ‘å€‘çš„æ–¹æ³•åŒ¹é…æˆ–è¶…éäº†æ²’æœ‰æ¨™æº–åŒ–æˆ–æ‰‹å‹•æ¢¯åº¦çš„ç¾æœ‰æ–¹æ³•ã€‚

---

## âœ… å„ªé»
- **å¯¦ç¾æœ€å…ˆé€²çš„æ€§èƒ½ï¼Œç„¡éœ€æ­¸ä¸€åŒ–å±¤:**  é€™æ˜¯è©²æ–¹æ³•çš„ä¸»è¦å„ªå‹¢ï¼Œå…‹æœäº†è¨±å¤šæ·±åº¦å­¸ç¿’æ¨¡å‹å°æ­¸ä¸€åŒ–å±¤çš„ä¾è³´ã€‚
- **åŸºæ–¼æ¦‚ç‡æ¨¡å‹ï¼Œè€Œéå•Ÿç™¼å¼æ–¹æ³•:** èˆ‡å¸¸ç”¨çš„æ›¿ä»£æ¢¯åº¦æ–¹æ³•ä¸åŒï¼Œè©²æ–¹æ³•åŸºæ–¼å°å¸¶å™ªè²äºŒå…ƒç¶²çµ¡çš„æ¦‚ç‡æ¨¡å‹ï¼Œä½¿å¾—ç«¯åˆ°ç«¯æ¢¯åº¦å„ªåŒ–æˆç‚ºå¯èƒ½ï¼Œæ›´å…·ç†è«–åŸºç¤ã€‚
- **å¼•å…¥çµ±ä¸€çš„IW-STä¼°è¨ˆå™¨:**  æ­¤ä¼°è¨ˆå™¨æ³›åŒ–äº†ç›´é€šä¼°è¨ˆå™¨å’ŒåŸºæ–¼é¬†å¼›çš„ä¼°è¨ˆå™¨ï¼Œæä¾›äº†ä¸€å€‹æ›´é€šç”¨çš„æ¡†æ¶ã€‚
- **ç‰¹å¾µåŒ–åå·®-æ–¹å·®æ¬Šè¡¡ä¸¦æœ€å°åŒ–åå·®:**  é€šéåˆ†æåå·®-æ–¹å·®æ¬Šè¡¡ä¸¦å¼•å…¥è¼”åŠ©æå¤±å‡½æ•¸ï¼Œæ¸›å°‘äº†æ¢¯åº¦ä¼°è¨ˆçš„åå·®ã€‚
- **æˆåŠŸæ‡‰ç”¨æ–¼äºŒå…ƒå’Œè„ˆè¡ç¥ç¶“ç¶²çµ¡ (SBNNs):**  è©²æ–¹æ³•ä¸å±€é™æ–¼äºŒå…ƒç¶²çµ¡ï¼Œä¹ŸæˆåŠŸæ‡‰ç”¨æ–¼æ›´è¤‡é›œçš„è„ˆè¡ç¥ç¶“ç¶²çµ¡ã€‚
- **è²è‘‰æ–¯æ–¹æ³•å¸¶ä¾†æ­£å‰‡åŒ–å’Œdropout-like noise:**  è²è‘‰æ–¯æ–¹æ³•å…§åœ¨çš„æ­£å‰‡åŒ–ç‰¹æ€§æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸¦ä¸”å¼•å…¥äº†é¡ä¼¼dropoutçš„å™ªè²ï¼Œæœ‰åŠ©æ–¼é˜²æ­¢éæ“¬åˆã€‚
- **åœ¨å¤šå€‹æ•¸æ“šé›†ä¸Šå–å¾—äº†å„ªç§€çš„çµæœ:** åœ¨CIFAR-10, DVS Gestureå’ŒSHDæ•¸æ“šé›†ä¸Šéƒ½å–å¾—äº†èˆ‡ç¾æœ‰æ–¹æ³•ç›¸ç•¶æˆ–æ›´å¥½çš„çµæœã€‚


## âš ï¸ ç¼ºé» / é™åˆ¶
- **è¨ˆç®—è¤‡é›œåº¦:**  è²è‘‰æ–¯æ–¹æ³•é€šå¸¸æ¯”å‚³çµ±æ–¹æ³•è¨ˆç®—è¤‡é›œåº¦æ›´é«˜ï¼Œé€™å¯èƒ½æœƒé™åˆ¶å…¶æ‡‰ç”¨æ–¼å¤§å‹æ•¸æ“šé›†æˆ–æ›´è¤‡é›œçš„ç¶²çµ¡çµæ§‹ã€‚
- **è¶…åƒæ•¸é¸æ“‡:**  é›–ç„¶æ–‡ä¸­æåˆ°é¿å…äº†æ›¿ä»£æ¢¯åº¦æ–¹æ³•ä¸­å°è¶…åƒæ•¸çš„æ•æ„Ÿæ€§ï¼Œä½†è©²æ–¹æ³•æœ¬èº«å¯èƒ½ä»ç„¶å­˜åœ¨éœ€è¦èª¿æ•´çš„è¶…åƒæ•¸ï¼Œåªæ˜¯æ•¸é‡å¯èƒ½è¼ƒå°‘ã€‚
- **å¯æ“´å±•æ€§:**  é›–ç„¶åœ¨æ–‡ä¸­æåˆ°çš„æ•¸æ“šé›†ä¸Šå–å¾—äº†æˆåŠŸï¼Œä½†è©²æ–¹æ³•çš„å¯æ“´å±•æ€§åœ¨æ›´å¤§è¦æ¨¡çš„æ•¸æ“šé›†å’Œæ›´è¤‡é›œçš„ä»»å‹™ä¸Šä»éœ€é©—è­‰ã€‚
- **å°å…ˆé©—åˆ†ä½ˆçš„é¸æ“‡:** è²è‘‰æ–¯æ–¹æ³•çš„æ€§èƒ½é«˜åº¦ä¾è³´æ–¼å…ˆé©—åˆ†ä½ˆçš„é¸æ“‡ï¼Œä¸æ°ç•¶çš„å…ˆé©—åˆ†ä½ˆå¯èƒ½æœƒå½±éŸ¿æ¨¡å‹çš„æ€§èƒ½ã€‚
- **ç¼ºä¹å°IW-STä¼°è¨ˆå™¨åå·®æœ€å°åŒ–æ–¹æ³•çš„æ·±å…¥è§£é‡‹:** æ‘˜è¦ä¸­åªç°¡è¦æåˆ°äº†åå·®æœ€å°åŒ–ï¼Œç¼ºä¹æ›´è©³ç´°çš„ç†è«–åˆ†æå’Œè§£é‡‹ã€‚

---
ï¼ˆç”± AI æŠ€è¡“ç ”ç©¶åŠ©æ‰‹è‡ªå‹•ç”¢ç”Ÿï¼‰
- **ä½œè€…**ï¼šJames A. Walker, Moein Khajehnejad, Adeel Razi
- **æ–¹æ³•æ‘˜è¦**ï¼šWe propose a Bayesian framework for training binary and spiking neural networks that achieves state-of-the-art performance without normalisation layers. Unlike commonly used surrogate gradient methods -- often heuristic and sensitive to hyperparameter choices -- our approach is grounded in a probabilistic model of noisy binary networks, enabling fully end-to-end gradient-based optimisation. We introduce importance-weighted straight-through (IW-ST) estimators, a unified class generalising straight-through and relaxation-based estimators. We characterise the bias-variance trade-off in this family and derive a bias-minimising objective implemented via an auxiliary loss. Building on this, we introduce Spiking Bayesian Neural Networks (SBNNs), a variational inference framework that uses posterior noise to train Binary and Spiking Neural Networks with IW-ST. This Bayesian approach minimises gradient bias, regularises parameters, and introduces dropout-like noise. By linking low-bias conditions, vanishing gradients, and the KL term, we enable training of deep residual networks without normalisation. Experiments on CIFAR-10, DVS Gesture, and SHD show our method matches or exceeds existing approaches without normalisation or hand-tuned gradients.
- **å„ªé»èˆ‡é™åˆ¶**ï¼š

## Leveraging KANs for Expedient Training of Multichannel MLPs via
  Preconditioning and Geometric Refinement

- **ä½œè€…**ï¼šJonas A. Actor, Graham Harper, Ben Southworth, Eric C. Cyr
- **ç™¼è¡¨æ—¥æœŸ**ï¼š2025-05-23
- **arXiv ID**ï¼š2505.18131

---

## ğŸ§  Abstractï¼ˆè‹±æ–‡ï¼‰

Multilayer perceptrons (MLPs) are a workhorse machine learning architecture, used in a variety of modern deep learning frameworks. However, recently Kolmogorov-Arnold Networks (KANs) have become increasingly popular due to their success on a range of problems, particularly for scientific machine learning tasks. In this paper, we exploit the relationship between KANs and multichannel MLPs to gain structural insight into how to train MLPs faster. We demonstrate the KAN basis (1) provides geometric localized support, and (2) acts as a preconditioned descent in the ReLU basis, overall resulting in expedited training and improved accuracy. Our results show the equivalence between free-knot spline KAN architectures, and a class of MLPs that are refined geometrically along the channel dimension of each weight tensor. We exploit this structural equivalence to define a hierarchical refinement scheme that dramatically accelerates training of the multi-channel MLP architecture. We show further accuracy improvements can be had by allowing the $1$D locations of the spline knots to be trained simultaneously with the weights. These advances are demonstrated on a range of benchmark examples for regression and scientific machine learning.

## ğŸŒ Abstractï¼ˆç¹é«”ç¿»è­¯ï¼‰

å¤šå±¤æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰æ˜¯ä¸€ç¨®ä¸»åŠ›æ©Ÿå™¨å­¸ç¿’é«”ç³»çµæ§‹ï¼Œç”¨æ–¼å„ç¨®ç¾ä»£æ·±åº¦å­¸ç¿’æ¡†æ¶ã€‚ä½†æ˜¯ï¼Œæœ€è¿‘ï¼ŒKolmogorov-Arnold Networksï¼ˆKANSï¼‰ç”±æ–¼åœ¨ä¸€ç³»åˆ—å•é¡Œä¸Šçš„æˆåŠŸè€Œè®Šå¾—è¶Šä¾†è¶Šå—æ­¡è¿ï¼Œç‰¹åˆ¥æ˜¯å°æ–¼ç§‘å­¸æ©Ÿå™¨å­¸ç¿’ä»»å‹™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å€‘åˆ©ç”¨KANSå’Œå¤šé€šé“MLPä¹‹é–“çš„é—œä¿‚ï¼Œä»¥ç²å–å¦‚ä½•æ›´å¿«åœ°è¨“ç·´MLPçš„çµæ§‹æ´å¯ŸåŠ›ã€‚æˆ‘å€‘è­‰æ˜äº†KANåŸºç¤ï¼ˆ1ï¼‰æä¾›äº†å¹¾ä½•å±€éƒ¨æ”¯æŒï¼Œä¸¦ä¸”ï¼ˆ2ï¼‰åœ¨RELUåŸºç¤ä¸Šå……ç•¶é è™•ç†çš„ä¸‹é™ï¼Œç¸½é«”ä¸Šå°è‡´äº†åŠ å¿«è¨“ç·´ä¸¦æé«˜æº–ç¢ºæ€§ã€‚æˆ‘å€‘çš„çµæœè¡¨æ˜ï¼Œè‡ªç”±çµæ¨£æ¢kané«”ç³»çµæ§‹èˆ‡ä¸€é¡MLPä¹‹é–“çš„ç­‰æ•ˆæ€§ï¼Œé€™äº›MLPæ²¿æ¯å€‹é‡é‡å¼µé‡çš„é€šé“å°ºå¯¸é€²è¡Œäº†å¹¾ä½•å½¢å¼ã€‚æˆ‘å€‘åˆ©ç”¨é€™ç¨®çµæ§‹å°ç­‰åº¦ä¾†å®šç¾©å±¤æ¬¡çµæ§‹çš„ç²¾ç…‰æ–¹æ¡ˆï¼Œè©²æ–¹æ¡ˆå¤§å¤§åŠ é€Ÿäº†å°å¤šæ¸ é“MLPé«”ç³»çµæ§‹çš„è¨“ç·´ã€‚æˆ‘å€‘å¯ä»¥é€šéå…è¨±$ 1 $ dçš„ä½ç½®èˆ‡é‡é‡åŒæ™‚è¨“ç·´ï¼Œæˆ‘å€‘å¯ä»¥é¡¯ç¤ºé€²ä¸€æ­¥çš„æº–ç¢ºæ€§æé«˜ã€‚é€™äº›é€²æ­¥åœ¨ç”¨æ–¼å›æ­¸å’Œç§‘å­¸æ©Ÿå™¨å­¸ç¿’çš„ä¸€ç³»åˆ—åŸºæº–ç¤ºä¾‹ä¸­å¾—åˆ°äº†è­‰æ˜ã€‚

---

## âœ… å„ªé»
- **åŠ é€Ÿ MLP è¨“ç·´:**  è«–æ–‡æå‡ºäº†ä¸€ç¨®åŸºæ–¼ Kolmogorov-Arnold Networks (KANs) çš„å±¤æ¬¡åŒ–ç´°åŒ–æ–¹æ¡ˆï¼Œé¡¯è‘—åŠ å¿«äº†å¤šé€šé“ MLP æ¶æ§‹çš„è¨“ç·´é€Ÿåº¦ã€‚
- **æå‡æº–ç¢ºæ€§:**  é€šéåˆ©ç”¨ KAN åŸºç¤çš„å¹¾ä½•å±€éƒ¨æ”¯æ’ç‰¹æ€§å’Œé æ¢ä»¶ä¸‹é™ç‰¹æ€§ï¼Œä»¥åŠå…è¨±ä¸€ç¶­æ¨£æ¢ç¯€é»ä½ç½®èˆ‡æ¬Šé‡åŒæ™‚è¨“ç·´ï¼Œæå‡äº† MLP æ¨¡å‹çš„æº–ç¢ºæ€§ã€‚
- **æä¾›çµæ§‹æ€§æ´å¯Ÿ:**  è«–æ–‡æ­ç¤ºäº† KANs èˆ‡å¤šé€šé“ MLP ä¹‹é–“çš„çµæ§‹ç­‰åƒ¹æ€§ï¼Œæä¾›äº†é—œæ–¼å¦‚ä½•æ›´å¿«è¨“ç·´ MLP çš„çµæ§‹æ€§è¦‹è§£ã€‚
- **é©ç”¨ç¯„åœå»£æ³›:**  è«–æ–‡åœ¨è¿´æ­¸å’Œç§‘å­¸æ©Ÿå™¨å­¸ç¿’çš„åŸºæº–ç¤ºä¾‹ä¸­è­‰æ˜äº†å…¶æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚


## âš ï¸ ç¼ºé» / é™åˆ¶
- **åƒ…é™æ–¼ç‰¹å®šé¡å‹çš„ MLP:**  è«–æ–‡çš„æˆæœå¯èƒ½åƒ…é©ç”¨æ–¼èˆ‡è‡ªç”±ç¯€é»æ¨£æ¢ KAN æ¶æ§‹ç­‰åƒ¹çš„ç‰¹å®šé¡å‹çš„å¤šé€šé“ MLPï¼Œå…¶é©ç”¨æ€§å¯èƒ½å­˜åœ¨é™åˆ¶ã€‚
- **è¨ˆç®—æˆæœ¬çš„æ½›åœ¨å¢åŠ :**  é›–ç„¶åŠ é€Ÿäº†è¨“ç·´ï¼Œä½†åŒæ™‚è¨“ç·´ç¯€é»ä½ç½®å’Œæ¬Šé‡å¯èƒ½æœƒå¢åŠ å–®æ¬¡è¿­ä»£çš„è¨ˆç®—æˆæœ¬ï¼Œéœ€è¦é€²ä¸€æ­¥è©•ä¼°å…¶æ•´é«”æ•ˆç‡ã€‚
- **å¯æ“´å±•æ€§:**  è«–æ–‡ä¸­æå‡ºçš„æ–¹æ³•åœ¨æ›´å¤§è¦æ¨¡çš„æ•¸æ“šé›†æˆ–æ›´è¤‡é›œçš„ä»»å‹™ä¸Šçš„å¯æ“´å±•æ€§å°šä¸æ¸…æ¥šï¼Œéœ€è¦æ›´å¤šå¯¦é©—é©—è­‰ã€‚
- **ç¼ºä¹å° KANs æœ¬èº«é™åˆ¶çš„è¨è«–:**  è«–æ–‡ä¸»è¦é—œæ³¨ KANs å¦‚ä½•æ”¹å–„ MLP è¨“ç·´ï¼Œä½†æ²’æœ‰æ·±å…¥æ¢è¨ KANs æœ¬èº«å¯èƒ½å­˜åœ¨çš„é™åˆ¶ï¼Œä¾‹å¦‚å°é«˜ç¶­æ•¸æ“šçš„è™•ç†èƒ½åŠ›ã€‚

---
ï¼ˆç”± AI æŠ€è¡“ç ”ç©¶åŠ©æ‰‹è‡ªå‹•ç”¢ç”Ÿï¼‰
- **ä½œè€…**ï¼šJonas A. Actor, Graham Harper, Ben Southworth, Eric C. Cyr
- **æ–¹æ³•æ‘˜è¦**ï¼šMultilayer perceptrons (MLPs) are a workhorse machine learning architecture, used in a variety of modern deep learning frameworks. However, recently Kolmogorov-Arnold Networks (KANs) have become increasingly popular due to their success on a range of problems, particularly for scientific machine learning tasks. In this paper, we exploit the relationship between KANs and multichannel MLPs to gain structural insight into how to train MLPs faster. We demonstrate the KAN basis (1) provides geometric localized support, and (2) acts as a preconditioned descent in the ReLU basis, overall resulting in expedited training and improved accuracy. Our results show the equivalence between free-knot spline KAN architectures, and a class of MLPs that are refined geometrically along the channel dimension of each weight tensor. We exploit this structural equivalence to define a hierarchical refinement scheme that dramatically accelerates training of the multi-channel MLP architecture. We show further accuracy improvements can be had by allowing the $1$D locations of the spline knots to be trained simultaneously with the weights. These advances are demonstrated on a range of benchmark examples for regression and scientific machine learning.
- **å„ªé»èˆ‡é™åˆ¶**ï¼š

